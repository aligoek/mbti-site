import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import torch
import os
import sys # sys modÃ¼lÃ¼nÃ¼ import ediyoruz

# NLTK verilerinin indirildiÄŸinden emin olun (sadece bir kez Ã§alÄ±ÅŸmasÄ± yeterli)
try:
    nltk.data.find('corpora/wordnet')
except LookupError: # DownloadError yerine LookupError kullanÄ±ldÄ±
    print("NLTK 'wordnet' kaynaÄŸÄ± bulunamadÄ±, indiriliyor...", file=sys.stderr)
    nltk.download('wordnet')
    print("'wordnet' baÅŸarÄ±yla indirildi.", file=sys.stderr)

try:
    nltk.data.find('corpora/stopwords')
except LookupError: # DownloadError yerine LookupError kullanÄ±ldÄ±
    print("NLTK 'stopwords' kaynaÄŸÄ± bulunamadÄ±, indiriliyor...", file=sys.stderr)
    nltk.download('stopwords')
    print("'stopwords' baÅŸarÄ±yla indirildi.", file=sys.stderr)

label_encoder = LabelEncoder()
mbti_types = [
    "ISTJ", "ISFJ", "INFJ", "INTJ", "ISTP", "ISFP", "INFP", "INTP",
    "ESTP", "ESFP", "ENFP", "ENTP", "ESTJ", "ESFJ", "ENFJ", "ENTJ",
]
label_encoder.fit(mbti_types)

# Modelin kaydedildiÄŸi yolu dinamik olarak belirleyelim
# Bu, pred.py dosyasÄ±nÄ±n bulunduÄŸu klasÃ¶rdeki 'models' klasÃ¶rÃ¼nÃ¼ iÅŸaret eder.
model_save_path = os.path.join(os.path.dirname(__file__), "models")
os.makedirs(model_save_path, exist_ok=True) # KlasÃ¶rÃ¼n var olduÄŸundan emin ol

best_model_path = os.path.join(model_save_path, "model.pth")

# Model dosyasÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
if not os.path.exists(best_model_path):
    print(f"Hata: Model dosyasÄ± bulunamadÄ±: {best_model_path}. LÃ¼tfen modelin eÄŸitildiÄŸinden ve doÄŸru yere kaydedildiÄŸinden emin olun.", file=sys.stderr)
    sys.exit(1) # Hata koduyla Ã§Ä±k

def preprocess_text(text):
    text = re.sub(r"[^\w\s]", "", text)
    words = text.split()
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words("english"))
    cleaned_text = [
        lemmatizer.lemmatize(word.lower())
        for word in words
        if word.lower() not in stop_words
    ]
    return " ".join(cleaned_text)

# Tokenizer ve modeli global olarak baÅŸlatÄ±n
# Bu, betik her Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda yeniden yÃ¼klemeyi Ã¶nler (ancak child_process her zaman yeniden baÅŸlatÄ±r).
try:
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-uncased", num_labels=len(label_encoder.classes_)
    )
    model.load_state_dict(torch.load(best_model_path, map_location=torch.device("cpu")))
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
except Exception as e:
    print(f"Model veya tokenizer yÃ¼klenirken hata oluÅŸtu: {e}", file=sys.stderr)
    sys.exit(1)

def predict_mbti(input_text):
    cleaned_text = preprocess_text(input_text)

    encoded_input = tokenizer.encode_plus(
        cleaned_text,
        add_special_tokens=True,
        max_length=64,
        padding="max_length",
        return_attention_mask=True,
        truncation=True,
        return_tensors="pt",
    )
    input_ids = encoded_input["input_ids"].to(device)
    attention_mask = encoded_input["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits

    predicted_label_idx = torch.argmax(logits, dim=1).cpu().numpy()[0]
    predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]
    return predicted_label

if __name__ == "__main__":
    # Komut satÄ±rÄ± argÃ¼manÄ± olarak metin alÄ±n
    if len(sys.argv) > 1:
        text_from_frontend = sys.argv[1]
        predicted_mbti = predict_mbti(text_from_frontend)
        print(predicted_mbti) # Sadece MBTI tipini yazdÄ±r
    else:
        # ArgÃ¼man olmadan doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda veya test iÃ§in Ã¶rnek metin
        sample_text = "2024 starts with a bang ğŸ˜…. Everyone's year-end summaries are so brilliant, compared to them it feels like I haven't lived at all. By contrast, I feel a year younger ğŸ˜˜.Today in class, I realized I lost my red pen. I remembered that my Python exam teacher borrowed it yesterday and didn't return it ğŸ˜….I only realized after the exam that there's a mode on the calculator that can calculate variance with just one click ğŸ˜…. When I asked my classmate how he knew, he said that calculators are allowed in Shanghai's college entrance exams, and they learned it quite early.During the trial, it was clearly stated that Trump had never been involved with Epstein Island. I'm surprised he didn't fabricate millions of pages of documents to drag Trump down, I'm devastated.Mariah Carey, you really have a discerning eye. At that time, you shot a music video for this song with a low-budget 'nobody cares' special effect, and indeed, this song has remained popular."
        predicted_mbti = predict_mbti(sample_text)
        print(predicted_mbti)
